{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anantapk03/farmer-rice-chatbot-model/blob/main/finetuningchatbotmodel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02Ur5oD1AUIj",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers datasets torch accelerate nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "# Dataset Class\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import DataLoader\n",
        "import tqdm\n",
        "import torch\n",
        "import csv\n",
        "import math\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "pqylgwOCEwWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "NFg7w9ag5BUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOTAL_TRIAL_DATA = 800\n",
        "TOTAL_EPOCH_TRIAL = 10"
      ],
      "metadata": {
        "id": "0dVQr_tp5eVj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **LOAD DATASET**"
      ],
      "metadata": {
        "id": "K1HYQn3HHwD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload semua file CSV\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "0yIbVbGAApPX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Tentukan jumlah file yang ingin dimuat (misal: 30 file dari 91 ke bawah)\n",
        "num_files_to_load = 8\n",
        "\n",
        "# Tentukan angka tertinggi\n",
        "start_number = 91\n",
        "\n",
        "# Hitung angka terendah berdasarkan selisih dari start_number\n",
        "end_number = start_number - num_files_to_load + 1 # +1 karena range tidak inklusif di akhir\n",
        "\n",
        "# Pastikan end_number tidak kurang dari 1 (jika memang file Anda bernomor dari 1)\n",
        "if end_number < 1:\n",
        "    end_number = 1\n",
        "\n",
        "# Buat list nama file secara dinamis\n",
        "# Range akan berjalan dari start_number hingga end_number (inklusif) secara mundur\n",
        "file_names = [f'{i}.csv' for i in range(start_number, end_number - 1, -1)]\n",
        "\n",
        "# Cetak file_names untuk memeriksa hasilnya (opsional)\n",
        "print(file_names)\n",
        "\n",
        "# Load semua CSV ke dalam DataFrame dan gabungkan\n",
        "dfs = [pd.read_csv(file) for file in file_names]\n",
        "df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "# Cek hasil gabungan data\n",
        "df.head()\n",
        "\n",
        "cleanedDataDuplicate = df.drop_duplicates(keep=\"first\")\n",
        "cleanedDataDuplicate.to_csv(\"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/\"+str(TOTAL_TRIAL_DATA)+\"/DATASET/dataset_\"+str(TOTAL_TRIAL_DATA)+\"_ROW.csv\", index=False)\n",
        "\n",
        "# Anggap 'cleanedDataDuplicate' adalah DataFrame hasil gabungan dan sudah bersih\n",
        "group_size = 11\n",
        "\n",
        "# Membagi DataFrame ke dalam list of groups, tiap group berisi 11 baris\n",
        "groups = [cleanedDataDuplicate.iloc[i:i+group_size] for i in range(0, len(cleanedDataDuplicate), group_size)]\n",
        "\n",
        "# Pastikan hanya group lengkap yang digunakan (jika sisa < 11 baris, diabaikan)\n",
        "groups = [g for g in groups if len(g) == group_size]\n",
        "\n",
        "# Split dengan proporsi 70% train, 20% valid, 10% test\n",
        "train_groups, temp_groups = train_test_split(groups, test_size=0.3, random_state=42)\n",
        "val_groups, test_groups = train_test_split(temp_groups, test_size=1/3, random_state=42)  # 1/3 dari 30% = 10%\n",
        "\n",
        "# Gabungkan kembali setiap list of groups ke satu DataFrame\n",
        "train_df = pd.concat(train_groups, ignore_index=True)\n",
        "val_df = pd.concat(val_groups, ignore_index=True)\n",
        "test_df = pd.concat(test_groups, ignore_index=True)\n",
        "\n",
        "# Simpan ke file\n",
        "train_df.to_csv(\"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/\"+str(TOTAL_TRIAL_DATA)+\"/DATASET/train.csv\", index=False)\n",
        "val_df.to_csv(\"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/\"+str(TOTAL_TRIAL_DATA)+\"/DATASET/valid.csv\", index=False)\n",
        "test_df.to_csv(\"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/\"+str(TOTAL_TRIAL_DATA)+\"/DATASET/test.csv\", index=False)\n",
        "\n",
        "print(cleanedDataDuplicate)"
      ],
      "metadata": {
        "id": "0-txf2hiRSFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLASS DATASET**"
      ],
      "metadata": {
        "id": "-5Zajkk-IUmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ChatData(Dataset):\n",
        "    def __init__(self, path: str, tokenizer, max_length: int = 150): # Reduced max_length\n",
        "        df = pd.read_csv(path)\n",
        "        df.dropna(subset=['pertanyaan', 'jawaban'], inplace=True)\n",
        "        df.drop_duplicates(subset=['pertanyaan', 'jawaban'], inplace=True)\n",
        "\n",
        "        self.X = [\n",
        "            f\"<startofstring> {row['pertanyaan']} <bot>: {row['jawaban']} <endofstring>\"\n",
        "            for _, row in df.iterrows()\n",
        "        ]\n",
        "\n",
        "        self.X_encoded = tokenizer(\n",
        "            self.X,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        self.input_ids = self.X_encoded['input_ids']\n",
        "        self.attention_mask = self.X_encoded['attention_mask']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attention_mask[idx]"
      ],
      "metadata": {
        "id": "KyawZS4-gnDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EVALUATION MODEL**"
      ],
      "metadata": {
        "id": "7ZYGXF4cI_7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_metrics(epoch, train_loss, val_loss, test_loss, train_perplexity, val_perplexity, test_perplexity, train_bleu_score, val_bleu_score, test_bleu_score):\n",
        "    with open(\"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/\"+str(TOTAL_TRIAL_DATA)+\"/EPOCH_\"+str(TOTAL_EPOCH_TRIAL)+\"/training_validation_log.csv\", \"a\", newline='') as f:\n",
        "        writer = csv.writer(f)\n",
        "        if f.tell() == 0:\n",
        "            # Tambahkan header baru\n",
        "            writer.writerow([\"epoch\", \"train_loss\", \"val_loss\", \"test_loss\",\n",
        "                             \"train_perplexity\", \"val_perplexity\", \"test_perplexity\",\n",
        "                             \"train_bleu_score\", \"val_bleu_score\", \"test_bleu_score\"])\n",
        "        # Ubah baris data\n",
        "        writer.writerow([epoch, train_loss, val_loss, test_loss,\n",
        "                         train_perplexity, val_perplexity, test_perplexity,\n",
        "                         train_bleu_score, val_bleu_score, test_bleu_score])\n",
        "\n",
        "def compute_bleu(preds, refs):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    scores = [\n",
        "        sentence_bleu([ref.split()], pred.split(), smoothing_function=smoothie)\n",
        "        for pred, ref in zip(preds, refs)\n",
        "    ]\n",
        "    return sum(scores) / len(scores)\n",
        "\n",
        "def evaluate(val_loader, model, tokenizer):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    preds = []\n",
        "    refs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, a in val_loader:\n",
        "            X, a = X.to(device), a.to(device)\n",
        "            outputs = model(X, attention_mask=a, labels=X)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Generate predictions\n",
        "            generated = model.generate(X, attention_mask=a, max_length=X.size(1),  max_new_tokens=150)\n",
        "            decoded_preds = [tokenizer.decode(g, skip_special_tokens=True) for g in generated]\n",
        "            decoded_refs = [tokenizer.decode(x, skip_special_tokens=True) for x in X]\n",
        "\n",
        "            preds.extend(decoded_preds)\n",
        "            refs.extend(decoded_refs)\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "    perplexity = math.exp(avg_loss)\n",
        "    bleu_score = compute_bleu(preds, refs)\n",
        "\n",
        "    return avg_loss, perplexity, bleu_score\n"
      ],
      "metadata": {
        "id": "4-G3B5TBA1W8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U bitsandbytes\n",
        "# # !rm -rf /root/.cache/huggingface/tokenizers/*\n",
        "# !pip install --upgrade transformers accelerate bitsandbytes"
      ],
      "metadata": {
        "id": "KQZawOEHEPmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TRAIN MODEL FUNCTION**"
      ],
      "metadata": {
        "id": "VC0VxureJjJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(train_loader, val_loader, test_loader, model, optim):\n",
        "    epochs = TOTAL_EPOCH_TRIAL\n",
        "    model.train()\n",
        "\n",
        "    for epoch in tqdm.tqdm(range(1, epochs + 1)):\n",
        "        total_train_loss = 0\n",
        "        train_batch_count = 0\n",
        "\n",
        "        # Inisialisasi untuk BLEU train\n",
        "        train_references = []\n",
        "        train_hypotheses = []\n",
        "\n",
        "        for X, a in train_loader:\n",
        "            X = X.to(device)\n",
        "            a = a.to(device)\n",
        "            optim.zero_grad()\n",
        "            loss = model(X, attention_mask=a, labels=X).loss\n",
        "            loss.backward()\n",
        "            optim.step()\n",
        "\n",
        "            total_train_loss += loss.item() # Gunakan total_train_loss\n",
        "            train_batch_count += 1 # Gunakan train_batch_count\n",
        "\n",
        "            # === Tambahan untuk menghitung BLEU train ===\n",
        "            # Generate predictions for BLEU calculation\n",
        "            with torch.no_grad(): # Gunakan no_grad() saat generate untuk BLEU train\n",
        "                outputs = model.generate(X, attention_mask=a, max_length=X.size(1), max_new_tokens=150) # Sesuaikan max_length\n",
        "            decoded_refs = [tokenizer.decode(x, skip_special_tokens=True) for x in X]\n",
        "            decoded_preds = [tokenizer.decode(g, skip_special_tokens=True) for g in outputs]\n",
        "\n",
        "            train_references.extend(decoded_refs)\n",
        "            train_hypotheses.extend(decoded_preds)\n",
        "\n",
        "            # === Akhir tambahan ===\n",
        "\n",
        "\n",
        "        # === Hitung Metrik untuk Train ===\n",
        "        avg_train_loss = total_train_loss / train_batch_count\n",
        "        train_perplexity = math.exp(avg_train_loss)\n",
        "        train_bleu_score = compute_bleu(train_hypotheses, train_references) # Hitung BLEU train\n",
        "\n",
        "\n",
        "        # === Evaluasi pada Validation Set ===\n",
        "        model.eval()\n",
        "        val_loss, val_perplexity, val_bleu_score = evaluate(val_loader, model, tokenizer) # Panggil evaluate untuk validation\n",
        "\n",
        "        # === Evaluasi pada Test Set ===\n",
        "        test_loss, test_perplexity, test_bleu_score = evaluate(test_loader, model, tokenizer) # Panggil evaluate untuk test\n",
        "\n",
        "        model.train() # Kembali ke mode train\n",
        "\n",
        "        # Logging (modifikasi parameter)\n",
        "        log_metrics(epoch, avg_train_loss, val_loss, test_loss, train_perplexity, val_perplexity, test_perplexity, train_bleu_score, val_bleu_score, test_bleu_score) # Tambahkan train_perplexity dan train_bleu_score\n",
        "\n",
        "        # Simpan model\n",
        "        torch.save(model.state_dict(), \"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/\"+str(TOTAL_TRIAL_DATA)+\"/EPOCH_\"+str(TOTAL_EPOCH_TRIAL)+\"/model_state.pt\")\n",
        "\n",
        "       # Output (modifikasi pencetakan)\n",
        "        print(f\"Epoch {epoch}\")\n",
        "        print(f\"Train Loss = {avg_train_loss:.4f}\")\n",
        "        print(f\"Val Loss   = {val_loss:.4f}\")\n",
        "        print(f\"Test Loss  = {test_loss:.4f}\")\n",
        "        print(f\"Train Perplexity = {train_perplexity:.4f}\")\n",
        "        print(f\"Val Perplexity = {val_perplexity:.4f}\")\n",
        "        print(f\"Test Perplexity = {test_perplexity:.4f}\") # Tambahkan print test perplexity\n",
        "        print(f\"Train BLEU Score = {train_bleu_score:.2f}\")\n",
        "        print(f\"Val BLEU Score = {val_bleu_score:.2f}\")\n",
        "        print(f\"Test BLEU Score = {test_bleu_score:.2f}\")\n",
        "\n",
        "def infer(inp):\n",
        "    inp = \"<startofstring> \"+inp\n",
        "    inp = tokenizer(inp, return_tensors=\"pt\")\n",
        "    X = inp[\"input_ids\"].to(device)\n",
        "    a = inp[\"attention_mask\"].to(device)\n",
        "    output = model.generate(X, attention_mask=a, max_new_tokens=150, eos_token_id=tokenizer.eos_token_id) # Contoh max_length\n",
        "    output = tokenizer.decode(output[0])\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "1Fib9hnPJL0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import huggingface_hub\n",
        "huggingface_hub.login(\"[YOUR_HUGGINGFACETOKENHERE]\")"
      ],
      "metadata": {
        "id": "B8lvFAmkKlXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **START**"
      ],
      "metadata": {
        "id": "8jv8djC1EYQ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "base_model = \"afrizalha/Bakpia-V1-0.5B-Javanese\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n",
        "                                \"bos_token\": \"<startofstring>\",\n",
        "                                \"eos_token\": \"<endofstring>\"})\n",
        "tokenizer.add_tokens([\"<bot>:\"])\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model) # Sesuaikan dengan tokenizer\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# Load dataset\n",
        "train_data = ChatData(\"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/\"+str(TOTAL_TRIAL_DATA)+\"/DATASET/train.csv\", tokenizer)\n",
        "val_data = ChatData(\"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/\"+str(TOTAL_TRIAL_DATA)+\"/DATASET/valid.csv\", tokenizer)\n",
        "test_data = ChatData(\"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/\"+str(TOTAL_TRIAL_DATA)+\"/DATASET/test.csv\", tokenizer) # Pastikan test_data dibuat\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32)\n",
        "test_loader = DataLoader(test_data, batch_size=32) # Buat test_loader\n",
        "\n",
        "model.train()\n",
        "\n",
        "optim = Adam(model.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "0H3V2qgHEW9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"training .... \")\n",
        "\n",
        "start_time = time.time() # Catat waktu mulai\n",
        "\n",
        "train(train_loader, val_loader, test_loader, model, optim)\n",
        "\n",
        "end_time = time.time() # Catat waktu selesai\n",
        "training_duration = end_time - start_time # Hitung durasi\n",
        "\n",
        "print(\"Successfully finetuning model!\")\n",
        "print(f\"Total training duration: {training_duration:.2f} seconds\") # Cetak durasi"
      ],
      "metadata": {
        "id": "Y7le0o8VA5HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VISUALIZE MODEL INFO**"
      ],
      "metadata": {
        "id": "_n7uV1oqEc-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_df = pd.read_csv(\"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/\"+str(TOTAL_TRIAL_DATA)+\"/EPOCH_\"+str(TOTAL_EPOCH_TRIAL)+\"/training_validation_log.csv\")\n",
        "\n",
        "plt.figure(figsize=(15, 10)) # Sesuaikan ukuran figure\n",
        "\n",
        "# Pilih semua kolom kecuali 'epoch'\n",
        "columns_to_average = log_df.columns.drop('epoch')\n",
        "\n",
        "# Hitung rata-rata untuk kolom-kolom tersebut\n",
        "average_values = log_df[columns_to_average].mean()\n",
        "\n",
        "# Tampilkan hasilnya\n",
        "print(\"Rata-rata nilai untuk setiap metrik:\")\n",
        "print(average_values)\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(2, 2, 1) # Ubah subplot menjadi 2x2\n",
        "plt.plot(log_df[\"epoch\"], log_df[\"train_loss\"], label=\"Train Loss\")\n",
        "plt.plot(log_df[\"epoch\"], log_df[\"val_loss\"], label=\"Val Loss\")\n",
        "plt.plot(log_df[\"epoch\"], log_df[\"test_loss\"], label=\"Test Loss\")\n",
        "plt.title(\"Train, Validation, and Test Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot Perplexity\n",
        "plt.subplot(2, 2, 2) # Plot perplexity di subplot kedua\n",
        "plt.plot(log_df[\"epoch\"], log_df[\"train_perplexity\"], label=\"Train Perplexity\")\n",
        "plt.plot(log_df[\"epoch\"], log_df[\"val_perplexity\"], label=\"Val Perplexity\")\n",
        "plt.plot(log_df[\"epoch\"], log_df[\"test_perplexity\"], label=\"Test Perplexity\")\n",
        "plt.title(\"Train, Validation, and Test Perplexity\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Perplexity\")\n",
        "plt.legend()\n",
        "\n",
        "# Plot BLEU Score\n",
        "plt.subplot(2, 2, 3) # Plot BLEU score di subplot ketiga\n",
        "plt.plot(log_df[\"epoch\"], log_df[\"train_bleu_score\"], label=\"Train BLEU\")\n",
        "plt.plot(log_df[\"epoch\"], log_df[\"val_bleu_score\"], label=\"Val BLEU\")\n",
        "plt.plot(log_df[\"epoch\"], log_df[\"test_bleu_score\"], label=\"Test BLEU\")\n",
        "plt.title(\"Train, Validation, and Test BLEU Score\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"BLEU Score\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout() # Mengatur layout agar tidak tumpang tindih\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OYw50PuiNhWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INFERENCE MACHINE**"
      ],
      "metadata": {
        "id": "wZJgypTREkJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TEST MODEL\n",
        "print(\"infer from model : \")\n",
        "while True:\n",
        "  inp = input()\n",
        "  print(infer(inp))"
      ],
      "metadata": {
        "id": "5e3lkxdoC6yp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7JtNQmDCGbWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a8bb152"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42b8df3e"
      },
      "source": [
        "### Instalasi Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "320dc43a"
      },
      "source": [
        "!pip install -q transformers datasets torch accelerate nltk rouge_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ae20943"
      },
      "source": [
        "### Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70ee5666"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4e5f087"
      },
      "source": [
        "### Load Tokenizer dan Model Arsitektur"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b92744a"
      },
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import pandas as pd\n",
        "import math\n",
        "import tqdm\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "from rouge_score import rouge_scorer\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Pastikan punkt tokenizer NLTK sudah diunduh\n",
        "nltk.download('punkt')\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"afrizalha/Bakpia-V1-0.5B-Javanese\")\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n",
        "                                \"bos_token\": \"<startofstring>\",\n",
        "                                \"eos_token\": \"<endofstring>\"})\n",
        "tokenizer.add_tokens([\"<bot>:\"])\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"afrizalha/Bakpia-V1-0.5B-Javanese\") # Sesuaikan dengan tokenizer\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a27caeff"
      },
      "source": [
        "### Load Model State Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9665cc29"
      },
      "source": [
        "# Tentukan path ke file model_state.pt yang sudah Anda simpan\n",
        "# Sesuaikan path ini dengan lokasi file Anda\n",
        "TOTAL_TRIAL_DATA = 800 # Ganti dengan nilai yang sesuai jika berbeda\n",
        "TOTAL_EPOCH_TRIAL = 10 # Ganti dengan nilai yang sesuai jika berbeda\n",
        "model_path = \"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/\"+str(TOTAL_TRIAL_DATA)+\"/EPOCH_\"+str(TOTAL_EPOCH_TRIAL)+\"/model_state.pt\"\n",
        "\n",
        "# Muat state dictionary model\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "# Set model ke mode evaluasi\n",
        "model.eval()\n",
        "\n",
        "print(\"Model state loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "474ebd08"
      },
      "source": [
        "### Siapkan Data Test dan DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26916a08"
      },
      "source": [
        "# Definisikan kembali kelas ChatData atau pastikan sudah tersedia di notebook ini\n",
        "class ChatData(Dataset):\n",
        "    def __init__(self, path: str, tokenizer, max_length: int = 150):\n",
        "        df = pd.read_csv(path)\n",
        "        df.dropna(subset=['pertanyaan', 'jawaban'], inplace=True)\n",
        "        df.drop_duplicates(subset=['pertanyaan', 'jawaban'], inplace=True)\n",
        "\n",
        "        self.X = [\n",
        "            f\"<startofstring> {row['pertanyaan']} <bot>: {row['jawaban']} <endofstring>\"\n",
        "            for _, row in df.iterrows()\n",
        "        ]\n",
        "\n",
        "        self.X_encoded = tokenizer(\n",
        "            self.X,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        self.input_ids = self.X_encoded['input_ids']\n",
        "        self.attention_mask = self.X_encoded['attention_mask']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attention_mask[idx]\n",
        "\n",
        "# Tentukan path ke file test.csv\n",
        "# Sesuaikan path ini dengan lokasi file Anda\n",
        "test_data_path = \"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/\"+str(TOTAL_TRIAL_DATA)+\"/DATASET/test.csv\"\n",
        "\n",
        "# Muat data test\n",
        "test_data = ChatData(test_data_path, tokenizer)\n",
        "test_loader = DataLoader(test_data, batch_size=32) # Sesuaikan batch size jika perlu\n",
        "\n",
        "print(\"Test data loaded successfully.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ae26306e"
      },
      "source": [
        "### Fungsi Evaluasi ROUGE dan METEOR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "613ce615"
      },
      "source": [
        "def compute_rouge_meteor(preds, refs):\n",
        "    \"\"\"\n",
        "    Menghitung skor ROUGE dan METEOR.\n",
        "\n",
        "    Args:\n",
        "        preds (list): List teks hipotesis (output model).\n",
        "        refs (list): List teks referensi (jawaban sebenarnya).\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary berisi skor ROUGE (rouge1, rouge2, rougel) dan METEOR.\n",
        "    \"\"\"\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "    meteor_scores = []\n",
        "\n",
        "    # Pastikan panjang preds dan refs sama\n",
        "    if len(preds) != len(refs):\n",
        "        print(\"Warning: Panjang prediksi dan referensi tidak sama.\")\n",
        "        min_len = min(len(preds), len(refs))\n",
        "        preds = preds[:min_len]\n",
        "        refs = refs[:min_len]\n",
        "\n",
        "\n",
        "    for pred, ref in zip(preds, refs):\n",
        "        # ROUGE Score\n",
        "        # Pastikan referensi dan prediksi tidak kosong sebelum menghitung skor\n",
        "        if ref and pred:\n",
        "            try:\n",
        "                scores = scorer.score(ref, pred) # Referensi sebagai argumen pertama\n",
        "                rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
        "                rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
        "                rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
        "            except Exception as e:\n",
        "                print(f\"Error calculating ROUGE for ref: '{ref}', pred: '{pred}' - {e}\")\n",
        "\n",
        "\n",
        "        # METEOR Score\n",
        "        # Tokenisasi untuk METEOR\n",
        "        ref_tokens = word_tokenize(ref)\n",
        "        pred_tokens = word_tokenize(pred)\n",
        "        # Pastikan token tidak kosong sebelum menghitung skor METEOR\n",
        "        if ref_tokens and pred_tokens:\n",
        "             try:\n",
        "                meteor_scores.append(meteor_score([ref_tokens], pred_tokens)) # Referensi sebagai list of lists\n",
        "             except Exception as e:\n",
        "                print(f\"Error calculating METEOR for ref: '{ref}', pred: '{pred}' - {e}\")\n",
        "\n",
        "\n",
        "    # Hitung rata-rata skor\n",
        "    avg_rouge_scores = {}\n",
        "    for key, value in rouge_scores.items():\n",
        "        if value: # Hindari pembagian oleh nol jika tidak ada skor yang berhasil dihitung\n",
        "            avg_rouge_scores[key] = sum(value) / len(value)\n",
        "        else:\n",
        "            avg_rouge_scores[key] = 0.0\n",
        "\n",
        "\n",
        "    if meteor_scores: # Hindari pembagian oleh nol jika tidak ada skor yang berhasil dihitung\n",
        "        avg_meteor_score = sum(meteor_scores) / len(meteor_scores)\n",
        "    else:\n",
        "        avg_meteor_score = 0.0\n",
        "\n",
        "\n",
        "    return {\"rouge\": avg_rouge_scores, \"meteor\": avg_meteor_score}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0a1fad2"
      },
      "source": [
        "### Jalankan Evaluasi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e377ad78"
      },
      "source": [
        "test_references = []\n",
        "test_hypotheses = []\n",
        "\n",
        "print(\"Generating predictions for test set...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for X, a in tqdm.tqdm(test_loader, desc=\"Evaluating\"): # Gunakan tqdm untuk progress bar\n",
        "        X, a = X.to(device), a.to(device)\n",
        "\n",
        "        # Generate predictions\n",
        "        # Sesuaikan parameter generate seperti max_new_tokens sesuai kebutuhan\n",
        "        generated = model.generate(\n",
        "            X,\n",
        "            attention_mask=a,\n",
        "            max_new_tokens=150, # Sesuaikan dengan kebutuhan\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.pad_token_id # Tambahkan pad_token_id\n",
        "            )\n",
        "\n",
        "        # Decode predictions and references\n",
        "        # Hapus token khusus saat decoding untuk evaluasi\n",
        "        decoded_preds = [tokenizer.decode(g, skip_special_tokens=True) for g in generated]\n",
        "        decoded_refs = [tokenizer.decode(x, skip_special_tokens=True) for x in X]\n",
        "\n",
        "\n",
        "        test_hypotheses.extend(decoded_preds)\n",
        "        test_references.extend(decoded_refs)\n",
        "\n",
        "\n",
        "print(\"Calculating ROUGE and METEOR scores...\")\n",
        "\n",
        "# Hitung skor ROUGE dan METEOR\n",
        "evaluation_results = compute_rouge_meteor(test_hypotheses, test_references)\n",
        "\n",
        "# Tampilkan hasilnya\n",
        "print(\"\\nHasil Evaluasi ROUGE dan METEOR pada Test Set:\")\n",
        "print(f\"ROUGE Scores: {evaluation_results['rouge']}\")\n",
        "print(f\"METEOR Score: {evaluation_results['meteor']:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44dea468"
      },
      "source": [
        "# Tentukan path untuk menyimpan hasil evaluasi ROUGE dan METEOR\n",
        "rouge_meteor_log_path = \"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/\"+str(TOTAL_TRIAL_DATA)+\"/EPOCH_\"+str(TOTAL_EPOCH_TRIAL)+\"/rouge_meteor_evaluation_log.csv\"\n",
        "\n",
        "# Buka file CSV dalam mode 'write' ('w')\n",
        "with open(rouge_meteor_log_path, \"w\", newline='') as f:\n",
        "    writer = csv.writer(f)\n",
        "\n",
        "    # Tulis header\n",
        "    writer.writerow([\"Metric\", \"Score\"])\n",
        "\n",
        "    # Tulis skor ROUGE\n",
        "    writer.writerow([\"ROUGE-1\", evaluation_results['rouge']['rouge1']])\n",
        "    writer.writerow([\"ROUGE-2\", evaluation_results['rouge']['rouge2']])\n",
        "    writer.writerow([\"ROUGE-L\", evaluation_results['rouge']['rougeL']])\n",
        "\n",
        "    # Tulis skor METEOR\n",
        "    writer.writerow([\"METEOR\", evaluation_results['meteor']])\n",
        "\n",
        "print(f\"Hasil evaluasi ROUGE dan METEOR telah disimpan ke: {rouge_meteor_log_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "565aff38"
      },
      "source": [
        "def train_summary(train_loader, model, optim):\n",
        "    \"\"\"\n",
        "    Fungsi training model yang disederhanakan, hanya menghitung loss per epoch.\n",
        "    \"\"\"\n",
        "    epochs = TOTAL_EPOCH_TRIAL # Gunakan konstanta yang sudah ada\n",
        "    model.train() # Pastikan model dalam mode training\n",
        "\n",
        "    for epoch in tqdm.tqdm(range(1, epochs + 1)):\n",
        "        total_train_loss = 0\n",
        "        train_batch_count = 0\n",
        "\n",
        "        for X, a in train_loader:\n",
        "            X = X.to(device) # Pindahkan data ke device yang sesuai\n",
        "            a = a.to(device) # Pindahkan data ke device yang sesuai\n",
        "            optim.zero_grad() # Reset gradien sebelumnya\n",
        "            loss = model(X, attention_mask=a, labels=X).loss # Hitung loss\n",
        "            loss.backward() # Lakukan backpropagation\n",
        "            optim.step() # Perbarui bobot model\n",
        "\n",
        "            total_train_loss += loss.item() # Akumulasikan loss per batch\n",
        "            train_batch_count += 1 # Hitung jumlah batch\n",
        "\n",
        "        # Hitung rata-rata loss untuk epoch ini\n",
        "        avg_train_loss = total_train_loss / train_batch_count\n",
        "\n",
        "        # Anda bisa mencetak atau menyimpan avg_train_loss jika perlu\n",
        "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f}\")\n",
        "\n",
        "        # Opsional: Simpan model state per epoch atau di akhir\n",
        "        # torch.save(model.state_dict(), f\"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/{TOTAL_TRIAL_DATA}/EPOCH_{TOTAL_EPOCH_TRIAL}/model_state_epoch_{epoch}.pt\")\n",
        "\n",
        "    print(\"Training process finished.\")\n",
        "\n",
        "# Catatan: Fungsi ini tidak akan menjalankan evaluasi pada validasi/test set\n",
        "# dan tidak akan menghitung metrik seperti Perplexity, BLEU, ROUGE, atau METEOR.\n",
        "# Ini hanya menunjukkan inti dari loop pelatihan."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}