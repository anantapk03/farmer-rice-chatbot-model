{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anantapk03/farmer-rice-chatbot-model/blob/main/BLEU_SCORE_EVAL_DETAIL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JadpKJ_FGaT9"
      },
      "outputs": [],
      "source": [
        "# Langkah 1: Setup Lingkungan\n",
        "!pip install -q transformers datasets torch accelerate nltk\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AutoTokenizer, AutoModelForCausalLM\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import csv\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"afrizalha/Bakpia-V1-0.5B-Javanese\" #change this base model\n",
        "\n",
        "# Tentukan device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Pastikan punkt tokenizer NLTK sudah diunduh\n",
        "nltk.download('punkt')\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\",\n",
        "                                \"bos_token\": \"<startofstring>\",\n",
        "                                \"eos_token\": \"<endofstring>\"})\n",
        "tokenizer.add_tokens([\"<bot>:\"])\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name) # Sesuaikan dengan tokenizer\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "TXBA_CprGbyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TOTAL_TRIAL_DATA = 10000 # Ganti dengan nilai yang sesuai jika berbeda\n",
        "TOTAL_EPOCH_TRIAL = 10 # Ganti dengan nilai yang sesuai jika berbeda\n",
        "model_path = \"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/\"+str(TOTAL_TRIAL_DATA)+\"/EPOCH_\"+str(TOTAL_EPOCH_TRIAL)+\"/model_state.pt\"\n",
        "\n",
        "# Muat state dictionary model\n",
        "model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "\n",
        "# Set model ke mode evaluasi\n",
        "model.eval()\n",
        "\n",
        "print(\"Model state loaded successfully.\")"
      ],
      "metadata": {
        "id": "lIecJTAZZGLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 3: Muat Dataset (Definisikan ulang ChatData jika belum ada di notebook ini)\n",
        "# Dataset Class\n",
        "class ChatData(Dataset):\n",
        "    def __init__(self, path: str, tokenizer, max_length: int = 150):\n",
        "        df = pd.read_csv(path)\n",
        "        df.dropna(subset=['pertanyaan', 'jawaban'], inplace=True)\n",
        "        df.drop_duplicates(subset=['pertanyaan', 'jawaban'], inplace=True)\n",
        "\n",
        "        self.X = [\n",
        "            f\"<startofstring> {row['pertanyaan']} <bot>: {row['jawaban']} <endofstring>\"\n",
        "            for _, row in df.iterrows()\n",
        "        ]\n",
        "\n",
        "        self.X_encoded = tokenizer(\n",
        "            self.X,\n",
        "            max_length=max_length,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        self.input_ids = self.X_encoded['input_ids']\n",
        "        self.attention_mask = self.X_encoded['attention_mask']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.attention_mask[idx]\n",
        "\n",
        "# Muat dataset\n",
        "train_data_path = f\"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/{TOTAL_TRIAL_DATA}/DATASET/train.csv\"\n",
        "val_data_path = f\"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/{TOTAL_TRIAL_DATA}/DATASET/valid.csv\"\n",
        "test_data_path = f\"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/{TOTAL_TRIAL_DATA}/DATASET/test.csv\"\n",
        "\n",
        "train_data = ChatData(train_data_path, tokenizer)\n",
        "val_data = ChatData(val_data_path, tokenizer)\n",
        "test_data = ChatData(test_data_path, tokenizer)\n",
        "\n",
        "print(\"Datasets loaded successfully.\")"
      ],
      "metadata": {
        "id": "7v3oLlU8GfMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 4: Siapkan Data Loaders\n",
        "batch_size = 32\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=False) # Tidak perlu shuffle untuk evaluasi\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"DataLoaders created.\")"
      ],
      "metadata": {
        "id": "D9CSVdYkGgJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "def compute_bleu(preds, refs):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    scores = []\n",
        "    for pred, ref in zip(preds, refs):\n",
        "        pred_tokens = pred.split()\n",
        "        ref_tokens = [ref.split()]\n",
        "        if pred_tokens:\n",
        "            score = sentence_bleu(ref_tokens, pred_tokens, smoothing_function=smoothie)\n",
        "        else:\n",
        "            score = 0.0\n",
        "        scores.append(score)\n",
        "    return scores  # Kembalikan list skor BLEU per sample\n",
        "\n",
        "\n",
        "def evaluate(data_loader, model, tokenizer, return_details=False):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    refs = []\n",
        "    questions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, a in tqdm.tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            X, a = X.to(device), a.to(device)\n",
        "\n",
        "            # Decode input untuk mendapatkan prompt sebelum <bot>:\n",
        "            decoded_inputs = [tokenizer.decode(x, skip_special_tokens=False) for x in X]\n",
        "\n",
        "            # Buat prompt yang hanya sampai <bot>:\n",
        "            prompts = []\n",
        "            reference_answers = []\n",
        "            extracted_questions = []\n",
        "\n",
        "            for text in decoded_inputs:\n",
        "                # Cari posisi tag\n",
        "                user_start = text.find(\"<startofstring>\")\n",
        "                bot_start = text.find(\"<bot>:\")\n",
        "                end_tag = text.find(\"<endofstring>\")\n",
        "\n",
        "                # Ambil prompt sampai <bot>: (untuk dijadikan input ke generate)\n",
        "                if user_start != -1 and bot_start != -1:\n",
        "                    prompt = text[user_start:bot_start + len(\"<bot>:\")]\n",
        "                    prompts.append(prompt.strip())\n",
        "\n",
        "                    # Ambil question (untuk logging)\n",
        "                    extracted_questions.append(text[user_start + len(\"<startofstring>\"):bot_start].strip())\n",
        "                else:\n",
        "                    prompts.append(\"\")  # fallback kosong\n",
        "                    extracted_questions.append(\"\")\n",
        "\n",
        "                # Ambil referensi jawaban dari <bot>: sampai <endofstring>\n",
        "                if bot_start != -1 and end_tag != -1:\n",
        "                    reference = text[bot_start + len(\"<bot>\"):end_tag].strip()\n",
        "                elif bot_start != -1:\n",
        "                    reference = text[bot_start + len(\"<bot>\"):].strip()\n",
        "                else:\n",
        "                    reference = \"\"\n",
        "                reference = reference.replace(\"<pad>\", \"\").replace(\"<startofstring>\", \"\").strip()\n",
        "                reference_answers.append(reference)\n",
        "\n",
        "            # Tokenisasi ulang prompt-only\n",
        "            prompt_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "            prompt_input_ids = prompt_tokens[\"input_ids\"].to(device)\n",
        "            prompt_attention_mask = prompt_tokens[\"attention_mask\"].to(device)\n",
        "\n",
        "            # Generate prediksi dari prompt saja\n",
        "            generated_outputs = model.generate(\n",
        "                input_ids=prompt_input_ids,\n",
        "                attention_mask=prompt_attention_mask,\n",
        "                max_new_tokens=150,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "            # Decode hasil generate\n",
        "            decoded_preds = [tokenizer.decode(g, skip_special_tokens=False) for g in generated_outputs]\n",
        "\n",
        "            # Ambil isi setelah <bot>: sampai <endofstring> dari prediksi\n",
        "            cleaned_preds = []\n",
        "            for pred_text in decoded_preds:\n",
        "                start_pred = pred_text.find(\"<bot>:\")\n",
        "                end_pred = pred_text.find(\"<endofstring>\")\n",
        "\n",
        "                if start_pred != -1:\n",
        "                    answer = pred_text[start_pred + len(\"<bot>\"): end_pred] if end_pred > start_pred else pred_text[start_pred + len(\"<bot>\"):]\n",
        "                    answer = answer.replace(\"<pad>\", \"\").replace(\"<startofstring>\", \"\").strip()\n",
        "                    cleaned_preds.append(answer)\n",
        "                else:\n",
        "                    cleaned_preds.append(\"\")\n",
        "\n",
        "            preds.extend(cleaned_preds)\n",
        "            refs.extend(reference_answers)\n",
        "            questions.extend(extracted_questions)\n",
        "\n",
        "    # Hitung BLEU per sample\n",
        "    bleu_scores_list = compute_bleu(preds, refs)\n",
        "    avg_bleu_score = sum(bleu_scores_list) / len(bleu_scores_list) if bleu_scores_list else 0.0\n",
        "\n",
        "    if return_details:\n",
        "        return questions, refs, preds, bleu_scores_list\n",
        "    else:\n",
        "        return avg_bleu_score\n"
      ],
      "metadata": {
        "id": "TouRT2rGq4E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 6 & 7: Lakukan Evaluasi dan Simpan Hasil\n",
        "import datetime # Import datetime\n",
        "\n",
        "print(\"Starting evaluation...\")\n",
        "\n",
        "# Lakukan evaluasi untuk setiap set data (rata-rata BLEU)\n",
        "train_bleu = evaluate(train_loader, model, tokenizer)\n",
        "val_bleu = evaluate(val_loader, model, tokenizer)\n",
        "\n",
        "# Lakukan evaluasi untuk data test dan dapatkan detail per baris\n",
        "test_questions, test_refs, test_preds, test_bleu_scores_list = evaluate(test_loader, model, tokenizer, return_details=True)\n",
        "\n",
        "# Hitung rata-rata BLEU untuk data test\n",
        "test_bleu = sum(test_bleu_scores_list) / len(test_bleu_scores_list) if test_bleu_scores_list else 0.0\n",
        "\n",
        "print(\"Evaluation finished.\")\n",
        "\n",
        "# Dapatkan tanggal saat ini\n",
        "current_date = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "# Simpan hasil rata-rata BLEU ke CSV\n",
        "results_path_avg = f\"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/{TOTAL_TRIAL_DATA}/EPOCH_{TOTAL_EPOCH_TRIAL}/bleu_evaluation_results_avg_{current_date}.csv\"\n",
        "\n",
        "results_df_avg = pd.DataFrame({\n",
        "    \"Dataset\": [\"Train\", \"Validation\", \"Test\"],\n",
        "    \"BLEU Score\": [train_bleu, val_bleu, test_bleu]\n",
        "})\n",
        "\n",
        "results_df_avg.to_csv(results_path_avg, index=False)\n",
        "\n",
        "print(f\"Average BLEU scores saved to {results_path_avg}\")\n",
        "\n",
        "# Simpan hasil detail BLEU untuk data test ke CSV\n",
        "results_path_test_details = f\"/content/drive/MyDrive/POLINDRA/SKRIPSI/chatbot_trial_finetuning/Komodo7B/{TOTAL_TRIAL_DATA}/EPOCH_{TOTAL_EPOCH_TRIAL}/bleu_evaluation_results_test_details_{current_date}.csv\"\n",
        "\n",
        "results_df_test_details = pd.DataFrame({\n",
        "    \"Pertanyaan\": test_questions,\n",
        "    \"Jawaban Referensi\": test_refs,\n",
        "    \"Jawaban Prediksi\": test_preds,\n",
        "    \"BLEU Score\": test_bleu_scores_list\n",
        "})\n",
        "\n",
        "results_df_test_details.to_csv(results_path_test_details, index=False)\n",
        "\n",
        "print(f\"Test dataset BLEU details saved to {results_path_test_details}\")\n",
        "\n",
        "\n",
        "# Langkah 8: Tampilkan Hasil\n",
        "print(\"\\n--- Average BLEU Scores ---\")\n",
        "print(results_df_avg)\n",
        "\n",
        "# Langkah 9: Finish task\n",
        "print(f\"\\nEvaluation complete. Average BLEU scores for Train, Validation, and Test datasets have been calculated and saved to {results_path_avg}.\")\n",
        "print(f\"Individual BLEU scores and details for the Test dataset have been saved to {results_path_test_details}.\")"
      ],
      "metadata": {
        "id": "U6RGwLAdGlfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9509781"
      },
      "source": [
        "def manual_inference(question, model, tokenizer, device, max_new_tokens=150):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Buat prompt yang sesuai format pelatihan\n",
        "        input_text = f\"<startofstring> {question} <bot>:\"\n",
        "        tokens = tokenizer(input_text, return_tensors='pt', padding=True)\n",
        "        input_ids = tokens[\"input_ids\"].to(device)\n",
        "        attention_mask = tokens[\"attention_mask\"].to(device)\n",
        "\n",
        "        # Generate\n",
        "        generated_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        # Decode\n",
        "        decoded_output = tokenizer.decode(generated_output[0], skip_special_tokens=False)\n",
        "\n",
        "        # Ambil jawaban dari <bot>: sampai <endofstring>\n",
        "        start_idx = decoded_output.find(\"<bot>:\")\n",
        "        if start_idx != -1:\n",
        "            response = decoded_output[start_idx + len(\"<bot>\"):]\n",
        "            end_idx = response.find(\"<endofstring>\")\n",
        "            if end_idx != -1:\n",
        "                response = response[:end_idx]\n",
        "            response = response.replace(\"<startofstring>\", \"\").replace(\"<pad>\", \"\").strip()\n",
        "        else:\n",
        "            response = \"Maaf, saya tidak dapat memahami pertanyaan Anda.\"\n",
        "\n",
        "        return response\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7650408e"
      },
      "source": [
        "# Contoh penggunaan manual inference\n",
        "question = \"Priben proses penyiapan lahan kanggo budidaya padi khusus?\"\n",
        "response = manual_inference(question, model, tokenizer, device)\n",
        "print(f\"Pertanyaan: {question}\")\n",
        "print(f\"Jawaban Bot: {response}\")\n",
        "\n",
        "#JAWABAN ACTUAL\n",
        "# Keuntungan nganggo biopestisida ning pengendalian hama lan penyakit tanduran yaiku murah lan bahan gampang didapat, ora menimbulkan residu ning tanaman, aman bagi manusia, hewan, lan ramah lingkungan, aman dinggo ning dosis tinggi, produk pertanian sing dihasilnang luwih sehat, ora gampang menyebab resistensi hama, kesehatan lema luwih terjaga, bisa ningkataken bahan organik tanah, bisa mempertahankan keberada\n",
        "\n",
        "# JAWABAN PREDIKSI TEST BLEU\n",
        "# Keuntungan nganggo biopestisida ning pengendalian hama lan penyakit tanduran yaiku murah lan bahan gampang didapat, ora menimbulkan residu ning tanaman, aman bagi manusia, hewan, lan ramah lingkungan, aman dinggo ning dosis tinggi, produk pertanian sing dihasilnang luwih sehat, ora gampang menyebab resistensi hama, kesehatan lema luwih terjaga, bisa ningkataken bahan organik tanah, bisa mempertahankan keberadaan sumber pestisida misale dadi sistem non PTT, lan nguntak siklus insektisida sejen."
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}